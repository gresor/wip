{
 "metadata": {
  "name": "",
  "signature": "sha256:83c4e8989cb574ea823a9b7ed6ff1174ceb84f5bf5b1a9d88f7f907caf5036fe"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "make a functions that makes a list of docs, fits a tfidf vecotizer to the text data.\n",
      "export the vecotizer and the tfidf matrix. then update the database with the sparce matrix tfidiv vextor for each doc."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.externals import joblib\n",
      "import pickle\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "from bson.objectid import ObjectId\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\n",
      "from sklearn.feature_selection import VarianceThreshold"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#starting up pymongo\n",
      "import pymongo\n",
      "#making mongo db of abstracts\n",
      "c = pymongo.MongoClient()\n",
      "db = c['PloS']\n",
      "abstract_db = db['this_year']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abstract_db.create_index('author_without_collab_display')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "u'author_without_collab_display_1'"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
      "from scipy.sparse import vstack"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#online trfidf\n",
      "cur = abstract_db.find({},{'abstract':1})\n",
      "hash_vect= HashingVectorizer(stop_words='english', ngram_range=(1,3), non_negative=True, norm=None)\n",
      "list_of_rows = []\n",
      "y=[]\n",
      "for doc in cur:\n",
      "    sparse_row = hash_vect.fit_transform(doc['abstract'])\n",
      "    list_of_rows.append(sparse_row)\n",
      "    y.append(str(doc['_id']))\n",
      "hash_stack = vstack(list_of_rows)\n",
      "\n",
      "h_transform = TfidfTransformer()\n",
      "hash_matrix = h_transform.fit_transform(hash_stack)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#export ids, and first 2 processing steps\n",
      "pickle.dump(y,open(\"ids.pkl\",\"wb\"))\n",
      "joblib.dump(hash_vect,'step_1.pkl')\n",
      "joblib.dump(h_transform,'step_2.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "['step_2.pkl', 'step_2.pkl_01.npy', 'step_2.pkl_02.npy']"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "select_feature =  VarianceThreshold(threshold=.00001)\n",
      "reduced_matrix = select_feature.fit_transform(hash_matrix)\n",
      "#joblib.dump(select_feature,'step_3.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#rnadomly select rows for random pca\n",
      "from sklearn.utils import resample"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reduced_matrix.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "2588"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random_sample = resample(reduced_matrix,replace =False, n_samples =1500)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cosine_pca= KernelPCA(n_components=1000,kernel='cosine')\n",
      "cosine_pca.fit(random_sample)\n",
      "topics = cosine_pca.transform(reduced_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cosine_pca= KernelPCA(n_components=1000,kernel='cosine')\n",
      "# topics = cosine_pca.fit_transform(reduced_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(topics,open(\"topics.pkl\",\"wb\"))\n",
      "joblib.dump(cosine_pca,'step_4.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "['step_4.pkl',\n",
        " 'step_4.pkl_01.npy',\n",
        " 'step_4.pkl_02.npy',\n",
        " 'step_4.pkl_03.npy',\n",
        " 'step_4.pkl_04.npy',\n",
        " 'step_4.pkl_05.npy',\n",
        " 'step_4.pkl_06.npy']"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "range(reduced_matrix.shape[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "[0,\n",
        " 1,\n",
        " 2,\n",
        " 3,\n",
        " 4,\n",
        " 5,\n",
        " 6,\n",
        " 7,\n",
        " 8,\n",
        " 9,\n",
        " 10,\n",
        " 11,\n",
        " 12,\n",
        " 13,\n",
        " 14,\n",
        " 15,\n",
        " 16,\n",
        " 17,\n",
        " 18,\n",
        " 19,\n",
        " 20,\n",
        " 21,\n",
        " 22,\n",
        " 23,\n",
        " 24,\n",
        " 25,\n",
        " 26,\n",
        " 27,\n",
        " 28,\n",
        " 29,\n",
        " 30,\n",
        " 31,\n",
        " 32,\n",
        " 33,\n",
        " 34,\n",
        " 35,\n",
        " 36,\n",
        " 37,\n",
        " 38,\n",
        " 39,\n",
        " 40,\n",
        " 41,\n",
        " 42,\n",
        " 43,\n",
        " 44,\n",
        " 45,\n",
        " 46,\n",
        " 47,\n",
        " 48,\n",
        " 49,\n",
        " 50,\n",
        " 51,\n",
        " 52,\n",
        " 53,\n",
        " 54,\n",
        " 55,\n",
        " 56,\n",
        " 57,\n",
        " 58,\n",
        " 59,\n",
        " 60,\n",
        " 61,\n",
        " 62,\n",
        " 63,\n",
        " 64,\n",
        " 65,\n",
        " 66,\n",
        " 67,\n",
        " 68,\n",
        " 69,\n",
        " 70,\n",
        " 71,\n",
        " 72,\n",
        " 73,\n",
        " 74,\n",
        " 75,\n",
        " 76,\n",
        " 77,\n",
        " 78,\n",
        " 79,\n",
        " 80,\n",
        " 81,\n",
        " 82,\n",
        " 83,\n",
        " 84,\n",
        " 85,\n",
        " 86,\n",
        " 87,\n",
        " 88,\n",
        " 89,\n",
        " 90,\n",
        " 91,\n",
        " 92,\n",
        " 93,\n",
        " 94,\n",
        " 95,\n",
        " 96,\n",
        " 97,\n",
        " 98,\n",
        " 99,\n",
        " 100,\n",
        " 101,\n",
        " 102,\n",
        " 103,\n",
        " 104,\n",
        " 105,\n",
        " 106,\n",
        " 107,\n",
        " 108,\n",
        " 109,\n",
        " 110,\n",
        " 111,\n",
        " 112,\n",
        " 113,\n",
        " 114,\n",
        " 115,\n",
        " 116,\n",
        " 117,\n",
        " 118,\n",
        " 119,\n",
        " 120,\n",
        " 121,\n",
        " 122,\n",
        " 123,\n",
        " 124,\n",
        " 125,\n",
        " 126,\n",
        " 127,\n",
        " 128,\n",
        " 129,\n",
        " 130,\n",
        " 131,\n",
        " 132,\n",
        " 133,\n",
        " 134,\n",
        " 135,\n",
        " 136,\n",
        " 137,\n",
        " 138,\n",
        " 139,\n",
        " 140,\n",
        " 141,\n",
        " 142,\n",
        " 143,\n",
        " 144,\n",
        " 145,\n",
        " 146,\n",
        " 147,\n",
        " 148,\n",
        " 149,\n",
        " 150,\n",
        " 151,\n",
        " 152,\n",
        " 153,\n",
        " 154,\n",
        " 155,\n",
        " 156,\n",
        " 157,\n",
        " 158,\n",
        " 159,\n",
        " 160,\n",
        " 161,\n",
        " 162,\n",
        " 163,\n",
        " 164,\n",
        " 165,\n",
        " 166,\n",
        " 167,\n",
        " 168,\n",
        " 169,\n",
        " 170,\n",
        " 171,\n",
        " 172,\n",
        " 173,\n",
        " 174,\n",
        " 175,\n",
        " 176,\n",
        " 177,\n",
        " 178,\n",
        " 179,\n",
        " 180,\n",
        " 181,\n",
        " 182,\n",
        " 183,\n",
        " 184,\n",
        " 185,\n",
        " 186,\n",
        " 187,\n",
        " 188,\n",
        " 189,\n",
        " 190,\n",
        " 191,\n",
        " 192,\n",
        " 193,\n",
        " 194,\n",
        " 195,\n",
        " 196,\n",
        " 197,\n",
        " 198,\n",
        " 199,\n",
        " 200,\n",
        " 201,\n",
        " 202,\n",
        " 203,\n",
        " 204,\n",
        " 205,\n",
        " 206,\n",
        " 207,\n",
        " 208,\n",
        " 209,\n",
        " 210,\n",
        " 211,\n",
        " 212,\n",
        " 213,\n",
        " 214,\n",
        " 215,\n",
        " 216,\n",
        " 217,\n",
        " 218,\n",
        " 219,\n",
        " 220,\n",
        " 221,\n",
        " 222,\n",
        " 223,\n",
        " 224,\n",
        " 225,\n",
        " 226,\n",
        " 227,\n",
        " 228,\n",
        " 229,\n",
        " 230,\n",
        " 231,\n",
        " 232,\n",
        " 233,\n",
        " 234,\n",
        " 235,\n",
        " 236,\n",
        " 237,\n",
        " 238,\n",
        " 239,\n",
        " 240,\n",
        " 241,\n",
        " 242,\n",
        " 243,\n",
        " 244,\n",
        " 245,\n",
        " 246,\n",
        " 247,\n",
        " 248,\n",
        " 249,\n",
        " 250,\n",
        " 251,\n",
        " 252,\n",
        " 253,\n",
        " 254,\n",
        " 255,\n",
        " 256,\n",
        " 257,\n",
        " 258,\n",
        " 259,\n",
        " 260,\n",
        " 261,\n",
        " 262,\n",
        " 263,\n",
        " 264,\n",
        " 265,\n",
        " 266,\n",
        " 267,\n",
        " 268,\n",
        " 269,\n",
        " 270,\n",
        " 271,\n",
        " 272,\n",
        " 273,\n",
        " 274,\n",
        " 275,\n",
        " 276,\n",
        " 277,\n",
        " 278,\n",
        " 279,\n",
        " 280,\n",
        " 281,\n",
        " 282,\n",
        " 283,\n",
        " 284,\n",
        " 285,\n",
        " 286,\n",
        " 287,\n",
        " 288,\n",
        " 289,\n",
        " 290,\n",
        " 291,\n",
        " 292,\n",
        " 293,\n",
        " 294,\n",
        " 295,\n",
        " 296,\n",
        " 297,\n",
        " 298,\n",
        " 299,\n",
        " 300,\n",
        " 301,\n",
        " 302,\n",
        " 303,\n",
        " 304,\n",
        " 305,\n",
        " 306,\n",
        " 307,\n",
        " 308,\n",
        " 309,\n",
        " 310,\n",
        " 311,\n",
        " 312,\n",
        " 313,\n",
        " 314,\n",
        " 315,\n",
        " 316,\n",
        " 317,\n",
        " 318,\n",
        " 319,\n",
        " 320,\n",
        " 321,\n",
        " 322,\n",
        " 323,\n",
        " 324,\n",
        " 325,\n",
        " 326,\n",
        " 327,\n",
        " 328,\n",
        " 329,\n",
        " 330,\n",
        " 331,\n",
        " 332,\n",
        " 333,\n",
        " 334,\n",
        " 335,\n",
        " 336,\n",
        " 337,\n",
        " 338,\n",
        " 339,\n",
        " 340,\n",
        " 341,\n",
        " 342,\n",
        " 343,\n",
        " 344,\n",
        " 345,\n",
        " 346,\n",
        " 347,\n",
        " 348,\n",
        " 349,\n",
        " 350,\n",
        " 351,\n",
        " 352,\n",
        " 353,\n",
        " 354,\n",
        " 355,\n",
        " 356,\n",
        " 357,\n",
        " 358,\n",
        " 359,\n",
        " 360,\n",
        " 361,\n",
        " 362,\n",
        " 363,\n",
        " 364,\n",
        " 365,\n",
        " 366,\n",
        " 367,\n",
        " 368,\n",
        " 369,\n",
        " 370,\n",
        " 371,\n",
        " 372,\n",
        " 373,\n",
        " 374,\n",
        " 375,\n",
        " 376,\n",
        " 377,\n",
        " 378,\n",
        " 379,\n",
        " 380,\n",
        " 381,\n",
        " 382,\n",
        " 383,\n",
        " 384,\n",
        " 385,\n",
        " 386,\n",
        " 387,\n",
        " 388,\n",
        " 389,\n",
        " 390,\n",
        " 391,\n",
        " 392,\n",
        " 393,\n",
        " 394,\n",
        " 395,\n",
        " 396,\n",
        " 397,\n",
        " 398,\n",
        " 399,\n",
        " 400,\n",
        " 401,\n",
        " 402,\n",
        " 403,\n",
        " 404,\n",
        " 405,\n",
        " 406,\n",
        " 407,\n",
        " 408,\n",
        " 409,\n",
        " 410,\n",
        " 411,\n",
        " 412,\n",
        " 413,\n",
        " 414,\n",
        " 415,\n",
        " 416,\n",
        " 417,\n",
        " 418,\n",
        " 419,\n",
        " 420,\n",
        " 421,\n",
        " 422,\n",
        " 423,\n",
        " 424,\n",
        " 425,\n",
        " 426,\n",
        " 427,\n",
        " 428,\n",
        " 429,\n",
        " 430,\n",
        " 431,\n",
        " 432,\n",
        " 433,\n",
        " 434,\n",
        " 435,\n",
        " 436,\n",
        " 437,\n",
        " 438,\n",
        " 439,\n",
        " 440,\n",
        " 441,\n",
        " 442,\n",
        " 443,\n",
        " 444,\n",
        " 445,\n",
        " 446,\n",
        " 447,\n",
        " 448,\n",
        " 449,\n",
        " 450,\n",
        " 451,\n",
        " 452,\n",
        " 453,\n",
        " 454,\n",
        " 455,\n",
        " 456,\n",
        " 457,\n",
        " 458,\n",
        " 459,\n",
        " 460,\n",
        " 461,\n",
        " 462,\n",
        " 463,\n",
        " 464,\n",
        " 465,\n",
        " 466,\n",
        " 467,\n",
        " 468,\n",
        " 469,\n",
        " 470,\n",
        " 471,\n",
        " 472,\n",
        " 473,\n",
        " 474,\n",
        " 475,\n",
        " 476,\n",
        " 477,\n",
        " 478,\n",
        " 479,\n",
        " 480,\n",
        " 481,\n",
        " 482,\n",
        " 483,\n",
        " 484,\n",
        " 485,\n",
        " 486,\n",
        " 487,\n",
        " 488,\n",
        " 489,\n",
        " 490,\n",
        " 491,\n",
        " 492,\n",
        " 493,\n",
        " 494,\n",
        " 495,\n",
        " 496,\n",
        " 497,\n",
        " 498,\n",
        " 499,\n",
        " 500,\n",
        " 501,\n",
        " 502,\n",
        " 503,\n",
        " 504,\n",
        " 505,\n",
        " 506,\n",
        " 507,\n",
        " 508,\n",
        " 509,\n",
        " 510,\n",
        " 511,\n",
        " 512,\n",
        " 513,\n",
        " 514,\n",
        " 515,\n",
        " 516,\n",
        " 517,\n",
        " 518,\n",
        " 519,\n",
        " 520,\n",
        " 521,\n",
        " 522,\n",
        " 523,\n",
        " 524,\n",
        " 525,\n",
        " 526,\n",
        " 527,\n",
        " 528,\n",
        " 529,\n",
        " 530,\n",
        " 531,\n",
        " 532,\n",
        " 533,\n",
        " 534,\n",
        " 535,\n",
        " 536,\n",
        " 537,\n",
        " 538,\n",
        " 539,\n",
        " 540,\n",
        " 541,\n",
        " 542,\n",
        " 543,\n",
        " 544,\n",
        " 545,\n",
        " 546,\n",
        " 547,\n",
        " 548,\n",
        " 549,\n",
        " 550,\n",
        " 551,\n",
        " 552,\n",
        " 553,\n",
        " 554,\n",
        " 555,\n",
        " 556,\n",
        " 557,\n",
        " 558,\n",
        " 559,\n",
        " 560,\n",
        " 561,\n",
        " 562,\n",
        " 563,\n",
        " 564,\n",
        " 565,\n",
        " 566,\n",
        " 567,\n",
        " 568,\n",
        " 569,\n",
        " 570,\n",
        " 571,\n",
        " 572,\n",
        " 573,\n",
        " 574,\n",
        " 575,\n",
        " 576,\n",
        " 577,\n",
        " 578,\n",
        " 579,\n",
        " 580,\n",
        " 581,\n",
        " 582,\n",
        " 583,\n",
        " 584,\n",
        " 585,\n",
        " 586,\n",
        " 587,\n",
        " 588,\n",
        " 589,\n",
        " 590,\n",
        " 591,\n",
        " 592,\n",
        " 593,\n",
        " 594,\n",
        " 595,\n",
        " 596,\n",
        " 597,\n",
        " 598,\n",
        " 599,\n",
        " 600,\n",
        " 601,\n",
        " 602,\n",
        " 603,\n",
        " 604,\n",
        " 605,\n",
        " 606,\n",
        " 607,\n",
        " 608,\n",
        " 609,\n",
        " 610,\n",
        " 611,\n",
        " 612,\n",
        " 613,\n",
        " 614,\n",
        " 615,\n",
        " 616,\n",
        " 617,\n",
        " 618,\n",
        " 619,\n",
        " 620,\n",
        " 621,\n",
        " 622,\n",
        " 623,\n",
        " 624,\n",
        " 625,\n",
        " 626,\n",
        " 627,\n",
        " 628,\n",
        " 629,\n",
        " 630,\n",
        " 631,\n",
        " 632,\n",
        " 633,\n",
        " 634,\n",
        " 635,\n",
        " 636,\n",
        " 637,\n",
        " 638,\n",
        " 639,\n",
        " 640,\n",
        " 641,\n",
        " 642,\n",
        " 643,\n",
        " 644,\n",
        " 645,\n",
        " 646,\n",
        " 647,\n",
        " 648,\n",
        " 649,\n",
        " 650,\n",
        " 651,\n",
        " 652,\n",
        " 653,\n",
        " 654,\n",
        " 655,\n",
        " 656,\n",
        " 657,\n",
        " 658,\n",
        " 659,\n",
        " 660,\n",
        " 661,\n",
        " 662,\n",
        " 663,\n",
        " 664,\n",
        " 665,\n",
        " 666,\n",
        " 667,\n",
        " 668,\n",
        " 669,\n",
        " 670,\n",
        " 671,\n",
        " 672,\n",
        " 673,\n",
        " 674,\n",
        " 675,\n",
        " 676,\n",
        " 677,\n",
        " 678,\n",
        " 679,\n",
        " 680,\n",
        " 681,\n",
        " 682,\n",
        " 683,\n",
        " 684,\n",
        " 685,\n",
        " 686,\n",
        " 687,\n",
        " 688,\n",
        " 689,\n",
        " 690,\n",
        " 691,\n",
        " 692,\n",
        " 693,\n",
        " 694,\n",
        " 695,\n",
        " 696,\n",
        " 697,\n",
        " 698,\n",
        " 699,\n",
        " 700,\n",
        " 701,\n",
        " 702,\n",
        " 703,\n",
        " 704,\n",
        " 705,\n",
        " 706,\n",
        " 707,\n",
        " 708,\n",
        " 709,\n",
        " 710,\n",
        " 711,\n",
        " 712,\n",
        " 713,\n",
        " 714,\n",
        " 715,\n",
        " 716,\n",
        " 717,\n",
        " 718,\n",
        " 719,\n",
        " 720,\n",
        " 721,\n",
        " 722,\n",
        " 723,\n",
        " 724,\n",
        " 725,\n",
        " 726,\n",
        " 727,\n",
        " 728,\n",
        " 729,\n",
        " 730,\n",
        " 731,\n",
        " 732,\n",
        " 733,\n",
        " 734,\n",
        " 735,\n",
        " 736,\n",
        " 737,\n",
        " 738,\n",
        " 739,\n",
        " 740,\n",
        " 741,\n",
        " 742,\n",
        " 743,\n",
        " 744,\n",
        " 745,\n",
        " 746,\n",
        " 747,\n",
        " 748,\n",
        " 749,\n",
        " 750,\n",
        " 751,\n",
        " 752,\n",
        " 753,\n",
        " 754,\n",
        " 755,\n",
        " 756,\n",
        " 757,\n",
        " 758,\n",
        " 759,\n",
        " 760,\n",
        " 761,\n",
        " 762,\n",
        " 763,\n",
        " 764,\n",
        " 765,\n",
        " 766,\n",
        " 767,\n",
        " 768,\n",
        " 769,\n",
        " 770,\n",
        " 771,\n",
        " 772,\n",
        " 773,\n",
        " 774,\n",
        " 775,\n",
        " 776,\n",
        " 777,\n",
        " 778,\n",
        " 779,\n",
        " 780,\n",
        " 781,\n",
        " 782,\n",
        " 783,\n",
        " 784,\n",
        " 785,\n",
        " 786,\n",
        " 787,\n",
        " 788,\n",
        " 789,\n",
        " 790,\n",
        " 791,\n",
        " 792,\n",
        " 793,\n",
        " 794,\n",
        " 795,\n",
        " 796,\n",
        " 797,\n",
        " 798,\n",
        " 799,\n",
        " 800,\n",
        " 801,\n",
        " 802,\n",
        " 803,\n",
        " 804,\n",
        " 805,\n",
        " 806,\n",
        " 807,\n",
        " 808,\n",
        " 809,\n",
        " 810,\n",
        " 811,\n",
        " 812,\n",
        " 813,\n",
        " 814,\n",
        " 815,\n",
        " 816,\n",
        " 817,\n",
        " 818,\n",
        " 819,\n",
        " 820,\n",
        " 821,\n",
        " 822,\n",
        " 823,\n",
        " 824,\n",
        " 825,\n",
        " 826,\n",
        " 827,\n",
        " 828,\n",
        " 829,\n",
        " 830,\n",
        " 831,\n",
        " 832,\n",
        " 833,\n",
        " 834,\n",
        " 835,\n",
        " 836,\n",
        " 837,\n",
        " 838,\n",
        " 839,\n",
        " 840,\n",
        " 841,\n",
        " 842,\n",
        " 843,\n",
        " 844,\n",
        " 845,\n",
        " 846,\n",
        " 847,\n",
        " 848,\n",
        " 849,\n",
        " 850,\n",
        " 851,\n",
        " 852,\n",
        " 853,\n",
        " 854,\n",
        " 855,\n",
        " 856,\n",
        " 857,\n",
        " 858,\n",
        " 859,\n",
        " 860,\n",
        " 861,\n",
        " 862,\n",
        " 863,\n",
        " 864,\n",
        " 865,\n",
        " 866,\n",
        " 867,\n",
        " 868,\n",
        " 869,\n",
        " 870,\n",
        " 871,\n",
        " 872,\n",
        " 873,\n",
        " 874,\n",
        " 875,\n",
        " 876,\n",
        " 877,\n",
        " 878,\n",
        " 879,\n",
        " 880,\n",
        " 881,\n",
        " 882,\n",
        " 883,\n",
        " 884,\n",
        " 885,\n",
        " 886,\n",
        " 887,\n",
        " 888,\n",
        " 889,\n",
        " 890,\n",
        " 891,\n",
        " 892,\n",
        " 893,\n",
        " 894,\n",
        " 895,\n",
        " 896,\n",
        " 897,\n",
        " 898,\n",
        " 899,\n",
        " 900,\n",
        " 901,\n",
        " 902,\n",
        " 903,\n",
        " 904,\n",
        " 905,\n",
        " 906,\n",
        " 907,\n",
        " 908,\n",
        " 909,\n",
        " 910,\n",
        " 911,\n",
        " 912,\n",
        " 913,\n",
        " 914,\n",
        " 915,\n",
        " 916,\n",
        " 917,\n",
        " 918,\n",
        " 919,\n",
        " 920,\n",
        " 921,\n",
        " 922,\n",
        " 923,\n",
        " 924,\n",
        " 925,\n",
        " 926,\n",
        " 927,\n",
        " 928,\n",
        " 929,\n",
        " 930,\n",
        " 931,\n",
        " 932,\n",
        " 933,\n",
        " 934,\n",
        " 935,\n",
        " 936,\n",
        " 937,\n",
        " 938,\n",
        " 939,\n",
        " 940,\n",
        " 941,\n",
        " 942,\n",
        " 943,\n",
        " 944,\n",
        " 945,\n",
        " 946,\n",
        " 947,\n",
        " 948,\n",
        " 949,\n",
        " 950,\n",
        " 951,\n",
        " 952,\n",
        " 953,\n",
        " 954,\n",
        " 955,\n",
        " 956,\n",
        " 957,\n",
        " 958,\n",
        " 959,\n",
        " 960,\n",
        " 961,\n",
        " 962,\n",
        " 963,\n",
        " 964,\n",
        " 965,\n",
        " 966,\n",
        " 967,\n",
        " 968,\n",
        " 969,\n",
        " 970,\n",
        " 971,\n",
        " 972,\n",
        " 973,\n",
        " 974,\n",
        " 975,\n",
        " 976,\n",
        " 977,\n",
        " 978,\n",
        " 979,\n",
        " 980,\n",
        " 981,\n",
        " 982,\n",
        " 983,\n",
        " 984,\n",
        " 985,\n",
        " 986,\n",
        " 987,\n",
        " 988,\n",
        " 989,\n",
        " 990,\n",
        " 991,\n",
        " 992,\n",
        " 993,\n",
        " 994,\n",
        " 995,\n",
        " 996,\n",
        " 997,\n",
        " 998,\n",
        " 999,\n",
        " ...]"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from multiprocessing import Pool"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pare_trans(x):\n",
      "    return cosine_pca.transform(reduced_matrix[x])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start= time.time()\n",
      "cosine_pca.transform(reduced_matrix)\n",
      "end = time.time()\n",
      "print end - start"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4.05839085579\n"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p = Pool(2)\n",
      "start= time.time()\n",
      "foo =p.map(pare_trans, range(reduced_matrix.shape[0]))\n",
      "end = time.time()\n",
      "print end - start"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "14.2996549606\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chunk_size=100\n",
      "start= time.time()\n",
      "cosine_pca.transform(reduced_matrix[0:chunk_size])\n",
      "end = time.time()\n",
      "a = end - start\n",
      "print a\n",
      "start= time.time()\n",
      "for i in range(chunk_size):\n",
      "    cosine_pca.transform(reduced_matrix[i])\n",
      "end = time.time()\n",
      "b =end - start\n",
      "print b\n",
      "print b/a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.174783945084\n",
        "0.976838827133"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5.58883612946\n"
       ]
      }
     ],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chunks=[]\n",
      "ratios=[]\n",
      "for j in range(100):\n",
      "\n",
      "    chunk_size=5*j+1\n",
      "    chunks.append(chunk_size)\n",
      "    start= time.time()\n",
      "    cosine_pca.transform(reduced_matrix[0:chunk_size])\n",
      "    end = time.time()\n",
      "    a = end - start\n",
      "    #print a\n",
      "    start= time.time()\n",
      "    for i in range(chunk_size):\n",
      "        cosine_pca.transform(reduced_matrix[i])\n",
      "    end = time.time()\n",
      "    b =end - start\n",
      "    ratios.append(b/a)\n",
      "    #print b\n",
      "    #print b/a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reduced_matrix.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 110,
       "text": [
        "2588"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(chunks,ratios)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 108,
       "text": [
        "[<matplotlib.lines.Line2D at 0x7f4c8ec24890>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEACAYAAAB8nvebAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VOXVB/DfEQQElACyGESDGwRFcbeCGhYRF6CLG2+1\n4itai1W0rvCqpNZWrfoqta3WurXWai1uYO2roKSggGglICSIIEiVNYIYlpCEOe8fZ66zZJY7k3sn\nuZnf9/PJJ5M7d+48uTNz7rnnee4zoqogIqJg2aupG0BERJlj8CYiCiAGbyKiAGLwJiIKIAZvIqIA\nYvAmIgqgtMFbRPqKyKKon20icl0uGkdERIlJJuO8RWQvAF8COElV/+Nbq4iIKKVMyybDAaxi4CYi\nalqZBu+LAfzVj4YQEZF7rssmItIGVjLpr6qbfW0VERGl1DqDdc8G8O/4wC0inByFiCgLqirZPjaT\nsslYAM8naQB/VDFlypQmb0Nz+eG+4L7gvkj901iugreIdIB1Vr7c6GckIqJGc1U2UdUdAPb3uS1E\nROQSr7D0UElJSVM3odngvojgvojgvvBORhfpJNyAiHpRv6Hm6/XXgaFDgfbtm7olRC2HiEBz1GFJ\neerHPwY++qipW0FE0Ri8KaVt24B164BNm/zZfn09sHOnP9smaskYvCml5cvt92afLsv6y1+ACRP8\n2TZRS8bgTSlVVtpvv4L3ypXA6tX+bJuoJWPwppSWLwf239+/ssnnnwNffunPtolaMgZvSqmyEjj9\ndP8ybyd4pxuwpArs2eNPG4iCiMGbUspF8K6pAbZuTb3eyy8DP/yhP20gCiIGb0pq925g7VrgO9/x\np2xSXw+sXw8ceqiNaEll/vxI/Z2IGLwphZUrgYMPBg480J/Me906oFs3oE+f9HXvRYusY5PXgxEZ\nBm9KqrISKC62DsuqKiAU8nb7n39uB4devVIHb1UL3m7KK0T5gsGbknKCd5s2QIcOwNdfe7t9t8F7\n7VqgXTugf38OKyRyMHhTUk7wBoDu3b0vnbgN3osWAcceCxQVAWvWeNsGoqBi8KakKiuBfv3sdrdu\n/gXvwsLUHZZO8O7Th5k3kYPBmxIKhYAVK2KDd/yIk1AIeOed7J9j7VrgoIPSZ94ffcTgTRSPwZsS\nWrsW6NwZ2G8/+ztR2aSiAhg2DHjmmeyeg2WT5GprgTvvbOpWUHPG4E0JRde7gcRlk9WrrRPxlluA\nhQsz276qHSAOPhjo0QPYsgWoq2u43ubNwPbtlnXnU+ZdWQn86lccGknJMXhTQomCd3zZZM0a4Iwz\ngD/+EfjBD4ANG9xvv6oKaNsW2HdfoFUry+zXr2+4npN1i0Qy73wIaBUVNh3Ajh1N3RJqrhi8KaHo\nzkogcdlk9WrLhseMAa64Ajj/fLsq0w2nZOJI1mnpBG/AAn379v5NktWcVFTYb45rp2QYvCmh5cvd\nlU369LHbd94J9OwJjB2buPwRLz54J6t7RwdvwJ4vH+rey5bZb6/H1lPTOuec9FNBuMXgTQm5KZtE\nB++99gKee84y70susXlLUnHq3Y5kwdsZaeIoKsqPundFhXUYM/POvXTv3Wxt2AAsWGBJjhcYvPPI\nli3AvfcCEycC//VfwKhRkW/KibZ+vdVbe/SILIsvm6jGBm/AatgvvWTZ4rhxqadwdZN5V1fbsujy\nTT50Wu7ebWcXJ54YzMx7505g3rymbkV2HnkEOOssf7Y9dy4weLAlOl5IuxkRKRCRaSJSKSIVInKK\nN09Njtpa/59j82b7BvglSywAnneezRY4ahTw1VeR9ZzMedw46yR0xM9vsnWrBfDOnWOfp1074NVX\n7QAweDDw618DH3/csJPx889tjLcjUfBevBg48kigdevIsnwYLvjpp/Z/9ujhX+a9YYOdXaXro9iz\nJ/M5be68E7jgguzb1lSqqoC77rLPyOLF2W2jthb4n/8B3n674X1z5gCnnda4NkZzcwyYCuANVS0G\ncDSAvJ6YUzXSmeRWXZ1lpC+/HBktsXs38MILwPDhNm/IVVdlfwXj0qX2pnvjjcSZ2oYNwJAhFrCf\new64/nrLvCdPBr77XetorK21D+m4cUBBAfDAA7HbiJ/fZM0aOwhEB3jHPvtYWyZPtiA9ZgwwYEBs\noHDTYRlf7wYSZ95//Svwr381bIeqZTuNGZ1SU2PT0eZSRYUdtDp39ifzLisDjjnGDtz77WdT8t59\nd+J1b7oJePBB99tevBj485+Bb76JTQqCoLQUuOgiOzP93e8yf/xnnwGDBgF//zvw+983vH/OHJsb\n3zOqmvQHQCcAn6VZR/PJggWqgOq0aenX3bZN9YEHVHv3Vj39dNXzzlMtLFTt0kW1a1fV4cNVX3hB\ndcMG1euvV91/f9WHH1ZdtUp17VrV9etVP/1U9eWXVX/+c9XLL1d9/XXVPXts+6GQ6m9/a4/76U9V\nhwxR7dhRdcAA1UsvVb3vPnts376qd92VuI319aqjRqmOH696ww2qgwer7tyZeN3DD1ddvtxuT5um\nOmaMu30WCqmedZbq009HlnXporpxY+TvigrVI46Ifdy4caqPPhq7rKLC2uHYs8f2b9euqh9/HLvu\nPffYa/WPf7hrZyKTJ6u2bav6xRfZbyNTd96pevvt9nvKlIb3r19vr+2kSao/+YnqxIm2j934059U\nu3dXnTXL/t69W/W111SPPz7x+qNG2fvJjT17VE85RfXxx1VPPVV19mx3j2sKy5fH7rNly+xztHmz\nfR4LClS3bo19zM6d9pmOV12t+uyzqt26qU6dqrppk+p++8V+jr76yj6btbWRZeHYmTIGp/pJF7wH\nAngfwNMAPgLwRwDtNY+D9zXXqF54ob3QS5YkX+/TT+1DMnas6ocfxt63fr0F53jLlqmee65qUZFq\nr172+IMPtmW33WaBfeBA1eJi1ccesw/W8cerfvJJZBu1taoLF6o++aQF4xEjVB96KPX/9M039gE9\n8kjVLVuSr3fqqapz59rt+++3A45bb75pzxEK2Zt9n31iPzzbtql26BBZtmePas+esf+bqn0g2raN\nHMDee0+1f3/78BQV2QdPVfUPf1Dt08faecYZ7tsZbcUKOyhceqnqhAnZbSMb55+v+te/2us2cWLD\n+x97TPW441TvvtsO3p06Rf7vaG+9pXrAAZYk3HCDHeCLiux9Fu2LL2y9RE44QbVVK9WlS9O3+7HH\nVAcNstfmxz9W/c1v0j/Gjd277XX8wx9UZ8xQXby4cdt7/32LfCNGqK5cacvOPlv1f/83ss7YsbGf\nm61bVfv1s/ftoYfaazR+vOoxx6i2b29Jz8KFkfVPP93a6pg+3V6HaH4H7xMA1AE4Mfz3wwDuiltH\np0yZ8u3P7OZ8uG2k3bstaH/2mepf/qJ6yCF2RI1XV2cZyNSp3rchFFJ9+23Lem+7zdrkhS1bUgdu\nVXvOl16y2xMmZPb/hUJ2cJg504JH374N1+nYUfXrr+32e++pHnVU4m317Kn6n//Y7YkT7axE1bLU\nk06y7LKw0A6gtbV2AFywwH1bnfaOHGlBY9MmO1NYsyazbWSrf3/V8nI7U/nRjxref/fddkbgOPVU\n1bKyhuvddZcF0X/+0zL1665LHOTr6lT33tvOwuL16mXJyu23p27zhg2WeToJze9+Z8HNCzNm2NnW\nFVeonnOOfQYbczZ19tn23v31r+3gfPnltv3oz9J776kedpgdiOrq7Mzx2mttH1VUqD73nOojj9iB\noKam4XM8+GDs/3/TTaqXXz47Jlb6Hbx7Algd9fdgAK/HrZP9XgyY116zI6zjxhtVzzzTXtxod9+t\nOmxYJDtsKcaPt+xK1T4A06dn9vgnnrDHvfGGZT3x+vaNZIU33mhlg0ROOcXOAPbssSBdWWnLQyHV\nSy6xTLS8PLL+1KmqP/hBZm195RU7w3FOcydPbhiMdu+2M4OFC+2g9O677ssXydTWqrZrp7prl+qr\nr9rZVbzrr4/NEq+4IvK6RBs7VvXPf3b3vD16qK5bF7usvt6C+rx5lm1G/2/19apXX6164omqBx5o\n60W/XnPmqJ58csPnWbXK1hs/3gLxxRcnDn7Rxo2zs07HPffYgSgbCxZYmc15ztWrVS+4wM4Mo4VC\ndpb7f/9nZy3Dhzf8nKeycqWdOTsHxJNPbniA9TV42/YxB8AR4dulAO7TFhq8v/km9Yfvggvs1M1R\nV2cfrhNOUP3gA1v24YeWgTiZYUsyeXKkdt6vX+qyUSK7dlmQuO461SuvbHj/kCEWBEMhK3lEB+Bo\nTlCaM0f16KNj76utVf3yy9hl27dbtrZihbt27thh2bpTF1a1M6yuXSOn2fPnW42+Tx8rXQ0datnb\n8OENa++ZiK7pl5WpnnZaw3UuuSQ2KCcrYR1zTOR9mc6xxzYs7znZdChkWWh0WeCRR6xEMn++nZHE\nB+AtW+xMKj6Buewy+xw99phl1GPGWCkymdpa2+/RZcaFC5OflaUzcmTDfpRknnjCAv3hh6c/K03k\nqKMsg6+utpJgfF9SLoL3MQA+ALAYwMsAOmkLCd61tdap8rOf2QvUtq11VJx8sh3tV62KrLt1q3VC\nxL+IoZDqM89YUJowwbK1557L6b+RMw89ZKeOoZDV/qqrM99Gaalq69Z2dhLvkktsXy5a1DDTizZp\nkh1Errkm8XYSueMOKyG4cd99VtOMN2WKHThuu81e7xdfjL2/ttbqvN26Wdt27Gi4jWXLLONMVu76\n+98jHcGLFycOUmedZWcvjhkzbFm0+vrMXqNzz42t0aqqfvRR5OB4xx32OVG1QNq1qx1oUundO/Yz\nFApZlh59EN261cqPf/tb4m289ZaVwqLV19vnNFEJqKZGtarK2vjJJ7HZ8vz5qgcdlD7Td+zYYQmF\n00mfqdtvV73lFktIBg1qeH9jg3faoYKqulhVT1TVY1T1+6q6LbPxLM2Dqg1FmzXLhtWdeSbQpQtw\n4402Z8YLLwC7dtkc1g88APTubdOdfvGFPX7aNBvWFz+uWQS47DIbM6tqQ4HGjs39/5cLziXyGzfa\nsMGOHTPfxk9+YhNRRY/xdjhjvV96Cfj+9xMPQwRsuODKlfaaXHihu+f96U+Bv/3N2p7Om2/akMl4\nN9xg9y1fbkPi4scy7703cO219l748kvg6qtjhynW1gI//KENgXziicTPXVFhMzUCNmQz0VDBqiob\nd+/o16/hxVZr1tjr5fY1OuCAhkM116+35QBw8cW2/0Ih25fXXht7BW4iAwbYmGnHypX2+7DDIssK\nCoAXXwSuucbGt8ebNs0mPYvWqpVNiDZ7duzytWttbPwRRwCnngqMGAEcfrgN29u1C/j5z234atu2\nqdvtaN/e5qvv29fd+vHGjLFrHjwfIuhoTOTXAGTeS5dajXTffa0GdfrpqjffbPXaRJ2N0e6/3+qw\nGzbY4155JTdtbq7eesvKA/PmWa0zW48/nrjz7ze/sbOX/v0tS0pm5kw7Czr22Myed9y49J2s9fX2\nXqmqSnz/11+7q2tv325Zc/Qp+m23WZntgw+sVp8oM7/oIusMV7UyXocODdc5+GDrNHfU1VmdPHp7\nM2ZYicCtRMMS//hH68xzHH20dRD36+cue7311tghqo8+mrgDVtVGzQwcGFtaqK+3z6xTqoo2dWrD\nPoi77mo4KmjePNXRo+1s6KCDvOvgdyMUsg7fgw6yTuN4aGTm3TpNbA+0qiq7EOGmmyxz6NIls8ff\ndJNdon3GGbatc87xp51B4WTe8ZfFZ+rKKxMv79ULePRRYNs24KSTkj++qMguArnoosye94QTYjPB\nRJYutQuGunZNfH+nTu6eq0MHuyhr0CDguOPsAqVnngHKyy07PPlkywhvuin2cRUVwK232u2OHe0i\nobo6y+od8Zl369aWza5YAQwcGNlOusw4WmGhzSMTLTrzBuwzNHmyZZJustejj7bM0/HOO3ahWCIT\nJgDvvmtnK888Y2dd775r7Tr00IbrDxsGPPxw5O9QCHj6abtAJtp3vgO89ppN9FVbaxeb5YqIZd+P\nPWZnAl5rEXOb1NTYixY9H3RtrZ1uXXSRvTEyDdyO0lI7hb/66ty+8M2RV8E7mcJCKzl873up5384\n6CB7LTK9BLu42Lafyvz59oH3wuGHA48/bu380Y/stjNfzC9+Adx/vx2EHPX1VlpwTtNFGpZOdu2y\n9eLLIfGlk8rKSPnFjURlk3Xr7DVxjB9vAdLtJd4DBtjUCIAF19mz7UrfRESslLR4MTB1qi1LVDJx\n9O9vc507V9v+619W/jzuuMTrH3lkw6t1c+GCC4BTTol8I5WXAh28a2psIpnDDrMX/KijgJ/9zOqa\nEyZYffqXv2zcc4jYN5oku3w4n3TrZlnfZ59Z9uu1Xr3s9/e/n3q9Nm2sDYccktn23QTvefO8C96A\nTT/w3/9tB6RRoyLLjzzSarLR2eOqVRYs27ePLIu/RP6rryzrju8P6Ncv9n/LJvNOVfMG7PVP1BeQ\nTN++VnvftcuCeOfO1peUTIcOlqnfey8wc6aduZx/fuJ1RWyuHuc7VJ96yvZzsn6SplJSYmcQfghs\n2WTFCjuKH3+8nRYdf7y92e691zosioqA997zbgYvisxvUl7uvqMwEwccYKfmbjp3nECfiZ49rQQR\nX3aINn8+cPPNmW87lSlTEi8vLbXySefOVhZJlC0XFMROTpWs7f36AdOn223VhlP6plNY2PCbjNav\nj828M9WmjZ19VFZaZjx0aPrHFBXZ4IHRoy3QR88oGW/YMAve558PzJgBPPRQ9m31k18HlMCGtttv\ntx7q6dMtcAP24Z861U4f33knu9EQlFq3bpGZCb3WujXw/POxswh6SSR19r15s81Znkm5oTEOPdQC\nziefWLlg586G/QHxmXeq4O2UTb780rL3TEqF3bvbtqPnsl63LjbzzoZTOnnnHQu2bpSU2Bn1Lbek\nXs/JvJ9/3kaPJTsgt1SBzLzLy222uKefTnx/Y99wlFz37nZ6Hz0jYJA4wTtR3XbBAsuEW7XKXXsu\nvdR+konPvDdvThyk+va1oXZ79mRe7wbsgNm1qx28CgutRr1xY+O/OGDAAOsInTPHShtuXXZZ+nUO\nOcSmIP7FL5IPvWzJApl533EHMGmSncJTbnXrZh9ut2Nlm5tUmbeXnZVecZt5d+xoyz//PPN6tyO6\ndFJVZZ1sjX2djz7aMuOiInvveG3oUDujGjHC+203d4EL3gsW2CnmVVc1dUvyU7du/pRMciVV8Pa6\ns9ILbmveQKR0kk3mDcSOOInvrMzWgAF2tuC2ZJKpq66yL/zI5dlScxG44H3HHfbTrl1TtyQ/de/u\nz0iTXEkWvOvrgX//24Z1NSfx32PpJng3JvN2gnf8MMFsHXigjY1301mZjZNPti8WyUeBqnmXldm4\nzkyGK5G3Ro6MHZscNEVFlglu3x7bob1kiY0fLyhosqYlVFAQ+81BVVXJyw/FxXbpfbaZd3TZxKvM\nW8S+WWf48MZvi2IFJvPesgW44grgvvtirzaj3DrtNODcc5u6Fdlr1cqGr33ySezy5ljvBjLPvOfO\nbfjl0W7Fl028yLwBG/bHM2XvNbvg/dlnNoFMTU1kWX29XSn5ve8lv+KKyK1EpZPmGrzjr7B0UzYp\nLs5ubHF82YSjtpq3ZhW8VW3WuRdftNqjkx3dcotdbHPvvU3bPmoZ4oN3KGRD2fyYf6Kx4jPvZEMF\nARvWt99+2Y9T96NsQv5pVjXvadPsiF9ebmO4Bw+2TPvtt4GFC/27eIPyS3GxXcXneOMNKzOkupqv\nqURn3qqWeSebNEvE/odsOiuB2LKJVx2W5J9mk3lXV9t8yY8+ajXtq66yoL16tc13ED+PNlG24jPv\nhx4Crr+++c2LAcRm3tXVNu46Vf34mmuyn/2ye3ebO6W+npl3EIhGzxafzQZEtLHbAOxLEbZsSX7V\nJJFXdu+24WvV1RbEzz7bkoTmOGtkXZ1d6l5ba20cNix29InXCguBDz6wqxe3bWNHo59EBKqadcrQ\nLAoRS5YAzz5rc+4S+a1tW5v0aOVKm9XvmmuaZ+AG7Cy0bVsb2piqs9IrBxxgc5F06MDA3dw1i+B9\n++3AnXf6c/ksUSLFxTbT3SuvJP76rebEuUQ+1RhvrxQW2sVKrHc3f00evNessalbozuQiPxWXGwJ\nw4UXNv/Z6JxL5HOReTvBm/Xu5q/JOywff9y+ZSR6AnoivxUX27C7iRObuiXpRWfeuSibMHgHQ5Nm\n3rt3A08+aWNsiXLpjDPsSxdyNXd3YziZd6ox3l4pLLRvYWfZpPlr0sz7pZds1jHnO/uIcqVPH5uN\nLgic4YK5KpsAzLyDwFXmLSJrAHwDYA+AOlVN8d3e7v3+9/adk0SUnHOhTq7KJgAz7yBwWzZRACWq\nusWrJ16yxDorR4/2aotELRMzb0okk7KJp9efPfqoXUXJS96JUstl5t29u80jxMy7+XMbvBXALBH5\nUESuTLt2GrW19tVI48c3dktELV8uM+9WrYBf/couYqLmzW3eO0hV14tINwAzRWS5qs517iwtLf12\nxZKSEpSUlKTc2AcfWIcRj+5E6RUU2JwjX3+d2TfCZ+vWW/1/jnxUVlaGsrIyz7aX8dwmIjIFwHZV\nfTD8d8Zzm9x9t2USDz6Y0cOI8tKcOcDVV9u3uX/1VVO3hrzS2LlN0pZNRKS9iOwbvt0BwAgAH2f7\nhAAwezYwZEhjtkCUPwoKgFWrmv+VoJRbbsomPQC8IjZfZmsAz6nqW9k+YU0N8P779nVaRJRe587W\nT8TgTdHSBm9VXQ1goFdPuGCBXdXWqZNXWyRq2Zy57Bm8KVrOr7CcPRsYOjTXz0oUXB062CgQBm+K\n1iTBm/VuIvdELPtm8KZoOQ3eO3cCH30EDBqUy2clCr6CAgZvipXT4P3ee8DAgUDHjrl8VqLg69yZ\nX1ZCsXIavFkyIcpOjx68qI1i5fQLiE85xS69ZYclUWZ27AD22cfmHaGWobEX6eQseFdX20xlmzfb\nm5CIKJ/5foWlV+bOBU48kYGbiMgLOQveM2cCw4fn6tmIiFq2nAXvWbMYvImIvJKTmveGDfZt3VVV\ndqUYEVG+C0TNe9YsGyLIwE1E5I2cBe8zz8zFMxER5Qffg7cqOyuJiLzme/BevhzYe2/gsMP8fiYi\novzhe/B2sm7x9LvniYjym+/Bm/VuIiLv+TpUsK7OprFcuZIzohERRWvWQwUXLgQOOYSBm4jIa74G\nb5ZMiIj84WvwXrzYJqMiIiJv+Rq8N24Eevb08xmIiPKTr8F70yb7BhAiIvKWq+AtIq1EZJGIzMhk\n4xs3At27Z9cwIiJKzm3mPRFABQDX4wp37QJqa4FOnbJqFxERpZA2eIvIgQDOAfAEANdjEjdtsqyb\nV1YSEXnPTeb9EICbAYQy2TBLJkRE/mmd6k4ROQ/AJlVdJCIlydYrLS399nZJSQlKSkqwcSM7K4mI\nHGVlZSgrK/NseykvjxeRXwG4FEA9gHYA9gPwkqr+KGqdhJfHP/EEMG8e8NRTnrWViKjF8PXyeFWd\nrKq9VbUPgIsBvBMduFPhMEEiIv9kOs7b9WgT1ryJiPzjOnir6r9UdbTb9VnzJiLyj29XWLJsQkTk\nH9+CNzNvIiL/+Bq8WfMmIvKHL9+kU1cHtG8P1NQArVo1avNERC1Ss/wmnaoqoGtXBm4iIr/4ErxZ\nMiEi8pdvwZudlURE/vEleHOYIBGRv5h5ExEFEGveREQBxLIJEVEAsWxCRBRALJsQEQUQM28iogDy\n/PL4UAho1w7Yvh1o06axzSMiapma3eXxW7cCHTsycBMR+cnz4M16NxGR/zwP3hwmSETkP18ybwZv\nIiJ/MXgTEQUQa95ERAHEmjcRUQCxbEJEFEBpg7eItBOR90WkXEQqROSeVOuzbEJE5L/W6VZQ1RoR\nGaKqO0WkNYB3RWSwqr6baH2WTYiI/OeqbKKqO8M32wBoBWBLsnWZeRMR+c9V8BaRvUSkHMBGALNV\ntSLRerW1QF0d0KGDl00kIqJ4acsmAKCqIQADRaQTgDdFpERVy5z7S0tLAQC7dgHt2pVApMT7lhIR\nBVhZWRnKyso8217GswqKyB0AdqnqA+G/v51V8PPPgdNOA9au9ax9REQtku+zCorI/iJSEL69D4Az\nASxKtG51NbDvvtk2hYiI3HJTNjkAwJ9EZC9YsH9WVd9OtOL27QzeRES54Gao4McAjnOzsepqm8ub\niIj85ekVlsy8iYhyw9PgzcybiCg3PA/ezLyJiPzHsgkRUQCxbEJEFEDMvImIAoiZNxFRALHDkogo\ngFg2ISIKIJZNiIgCiGUTIqIA8rxswsybiMh/zLyJiAKIHZZERAHkWfDesweoqQHat/dqi0RElIxn\nwXv7dvviYcn6S32IiMgtT4M3SyZERLnhWfDmGG8iotxh5k1EFEDMvImIAsjT4M3Mm4goN1g2ISIK\nIJZNiIgCKG3wFpHeIjJbRJaJyFIRuS7ReiybEBHlTmsX69QBuEFVy0WkI4B/i8hMVa2MXomTUhER\n5U7azFtVN6hqefj2dgCVAArj12PmTUSUOxnVvEWkCMCxAN6Pv48dlkREueOmbAIACJdMpgGYGM7A\nv1VaWop584DNm4G+fUtQUlLicTOJiIKtrKwMZWVlnm1PVDX9SiJ7A3gdwD9V9eG4+1RVMWoUcOWV\nwOjRnrWNiKjFEhGoatZT+bkZbSIAngRQER+4o7HDkogod9zUvAcBuATAEBFZFP4ZGb8SOyyJiHIn\nbc1bVd+FiyDPDksiotzhFZZERAHEiamIiALIk+CtCuzYYV+DRkRE/vMkeO/cCbRtC7R2PWqciIga\nw5PgzZIJEVFueRK8OcabiCi3mHkTEQUQM28iogBi5k1EFEAM3kREAcSyCRFRADHzJiIKIGbeREQB\nxMybiCiAGLyJiAKIZRMiogBi5k1EFECeBW9m3kREueNZ2YSZNxFR7rBsQkQUQOywJCIKIGbeREQB\nxA5LIqIAShu8ReQpEdkoIh8nX8e+w5KIiHLDTeb9NICRqVZgyYSIKLfSBm9VnQtga6p1WDIhIsot\nT2rezLwA5vACAAAE/ElEQVSJiHKrtRcb+frrUpSW2u2SkhKUlJR4sVkiohajrKwMZWVlnm1PVDX9\nSiJFAGao6oAE9+nw4YqZMz1rExFRiyciUFXJ9vEsmxARBZCboYLPA5gH4AgR+Y+IXB6/zqRJfjSN\niIiScVU2SbkBEW3sNoiI8k2zKJsQEVFuMXgTEQUQgzcRUQAxeBMRBRCDNxFRADF4ExEFEIM3EVEA\nMXgTEQUQgzcRUQAxeBMRBRCDNxFRADF4ExEFEIM3EVEAMXgTEQUQgzcRUQAxeBMRBRCDNxFRADF4\nExEFEIM3EVEAMXgTEQUQgzcRUQAxeBMRBVDa4C0iI0VkuYh8KiK35qJRRESUWsrgLSKtAPwWwEgA\n/QGMFZHiXDQsiMrKypq6Cc0G90UE90UE94V30mXeJwFYqaprVLUOwAsAxvjfrGDiGzOC+yKC+yKC\n+8I76YJ3LwD/ifr7i/AyIiJqQumCt+akFURElBFRTR6fReQUAKWqOjL89yQAIVW9L2odBngioiyo\nqmT72HTBuzWATwAMA7AOwEIAY1W1MtsnJCKixmud6k5VrReRnwJ4E0ArAE8ycBMRNb2UmTcRETVP\njbrCMt8u4BGRp0Rko4h8HLWsi4jMFJEVIvKWiBRE3TcpvG+Wi8iIpmm1P0Skt4jMFpFlIrJURK4L\nL8+7/SEi7UTkfREpF5EKEbknvDzv9gVg14eIyCIRmRH+Oy/3AwCIyBoRWRLeHwvDy7zZH6qa1Q+s\njLISQBGAvQGUAyjOdntB+AFwGoBjAXwctezXAG4J374VwL3h2/3D+2Tv8D5aCWCvpv4fPNwXPQEM\nDN/uCOsbKc7j/dE+/Ls1gAUABufxvvgZgOcATA//nZf7Ifw/rgbQJW6ZJ/ujMZl33l3Ao6pzAWyN\nWzwawJ/Ct/8E4Lvh22MAPK+qdaq6BvZCnJSLduaCqm5Q1fLw7e0AKmHXAOTr/tgZvtkGlthsRR7u\nCxE5EMA5AJ4A4IykyLv9ECd+RIkn+6MxwZsX8JgeqroxfHsjgB7h24WwfeJosftHRIpgZyTvI0/3\nh4jsJSLlsP95tqouQ37ui4cA3AwgFLUsH/eDQwHMEpEPReTK8DJP9kfK0SYuGkVRVFXTjHtvcftM\nRDoCeAnARFWtFokkGfm0P1Q1BGCgiHQC8KaIDIm7v8XvCxE5D8AmVV0kIiWJ1smH/RBnkKquF5Fu\nAGaKyPLoOxuzPxqTeX8JoHfU370Re9TIFxtFpCcAiMgBADaFl8fvnwPDy1oMEdkbFrifVdVXw4vz\ndn8AgKpuA/APAMcj//bFqQBGi8hqAM8DGCoizyL/9sO3VHV9+PdmAK/AyiCe7I/GBO8PARwuIkUi\n0gbARQCmN2J7QTUdwGXh25cBeDVq+cUi0kZE+gA4HHaRU4sglmI/CaBCVR+Ouivv9oeI7O+MGBCR\nfQCcCWAR8mxfqOpkVe2tqn0AXAzgHVW9FHm2Hxwi0l5E9g3f7gBgBICP4dX+aGRP6tmwUQYrAUxq\n6p5dv39g2cQ6ALWwev/lALoAmAVgBYC3ABRErT85vG+WAzirqdvv8b4YDKtrlsMC1SLY1MF5tz8A\nDADwUXhfLAFwc3h53u2LqP/vDERGm+TlfgDQJ/yeKAew1ImRXu0PXqRDRBRA/Bo0IqIAYvAmIgog\nBm8iogBi8CYiCiAGbyKiAGLwJiIKIAZvIqIAYvAmIgqg/wcb98CPQkvuqwAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f4cb44e9fd0>"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "range(0,length,chunksize):\n",
      "    if i plus chunk < length:\n",
      "        print %i:%i  %(i,i+chunk)\n",
      "    else:\n",
      "        i:"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rngs=[]\n",
      "chunk=100\n",
      "for i in range(0,reduced_matrix.shape[0],chunk):\n",
      "    if i + chunk < reduced_matrix.shape[0]:\n",
      "        r =(i,i+chunk)\n",
      "        rngs.append(r)\n",
      "    else:\n",
      "        r= (i,None)\n",
      "        rngs.append(r)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pare_trans(x):\n",
      "    return cosine_pca.transform(reduced_matrix[x[0]:x[1]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#reduced_matrix[rngs[0]]\n",
      "rngs[0][1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 134,
       "text": [
        "100"
       ]
      }
     ],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p = Pool(4)\n",
      "start= time.time()\n",
      "foo =p.map(pare_trans, rngs)\n",
      "end = time.time()\n",
      "print end - start"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2.53551101685\n"
       ]
      }
     ],
     "prompt_number": 169
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vfoo = np.vstack(foo)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 170
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 165
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(10):\n",
      "    x=random.randint(0, 2500)\n",
      "    print cosine_similarity(vfoo[x],topics[x])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.]]\n",
        "[[ 1.]]\n",
        "[[ 1.]]\n",
        "[[ 1.]]\n",
        "[[ 1.]]\n",
        "[[ 1.]]\n",
        "[[ 1.]]\n",
        "[[ 1.]]\n",
        "[[ 1.]]\n",
        "[[ 1.]]\n"
       ]
      }
     ],
     "prompt_number": 171
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "len(topics[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 172,
       "text": [
        "1000"
       ]
      }
     ],
     "prompt_number": 172
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(vfoo[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 174,
       "text": [
        "1000"
       ]
      }
     ],
     "prompt_number": 174
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.neural_network import BernoulliRBM"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bar = BernoulliRBM(n_components=1000,learning_rate=.001)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bar.fit(random_sample)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 76,
       "text": [
        "BernoulliRBM(batch_size=10, learning_rate=0.001, n_components=1000, n_iter=10,\n",
        "       random_state=None, verbose=0)"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cosine_similarity(bar.transform(reduced_matrix[40])[0],bar.transform(reduced_matrix[40])[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 79,
       "text": [
        "array([[ 1.]])"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cosine_similarity(topics[40],topics[40])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 80,
       "text": [
        "array([[ 1.]])"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "ignore below this point"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# topics.nbytes - r_topics\n",
      "topics"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "array([[ 0.00268863,  0.08744445,  0.00774153, ..., -0.00210854,\n",
        "        -0.00358399, -0.00309392],\n",
        "       [-0.05352065,  0.02928797, -0.01968799, ..., -0.04851551,\n",
        "        -0.00419859,  0.0326805 ],\n",
        "       [ 0.07469063, -0.00118776,  0.04121224, ...,  0.01244514,\n",
        "        -0.00780602,  0.00911737],\n",
        "       ..., \n",
        "       [-0.07494702,  0.08565954, -0.07317758, ...,  0.00537154,\n",
        "         0.00327966, -0.00446553],\n",
        "       [-0.02323088,  0.06345144, -0.00276649, ...,  0.00212175,\n",
        "        -0.00487512,  0.0046332 ],\n",
        "       [-0.01208251,  0.00572762,  0.06472528, ..., -0.01210626,\n",
        "        -0.00273896, -0.00089529]])"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shit =[]\n",
      "for i in topics:\n",
      "    shit.append(i)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "howbig = cosine_pca.transform(reduced_matrix[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print howbig.nbytes, topics[0].nbytes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8000 8000\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cosine_pca= KernelPCA(n_components=1000,kernel='cosine')\n",
      "# topics = cosine_pca.fit_transform(hash_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "joblib.dump(cosine_pca,'step_4.pkl')\n",
      "pickle.dump(topics,open(\"topics.pkl\",\"wb\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kn =KNeighborsClassifier(algorithm= 'brute',metric='cosine')\n",
      "kn.fit(topics,y)\n",
      "joblib.dump(kn,'kn_search.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "['kn_search.pkl',\n",
        " 'kn_search.pkl_01.npy',\n",
        " 'kn_search.pkl_02.npy',\n",
        " 'kn_search.pkl_03.npy']"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kn_shit =KNeighborsClassifier(algorithm= 'brute',metric='cosine')\n",
      "kn_shit.fit(shit,y)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='cosine',\n",
        "           metric_params=None, n_neighbors=5, p=2, weights='uniform')"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lshash import LSHash"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsh =LSHash(32,1000,2)\n",
      "lsh.index(topics)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('The input point needs to be of the same dimension as\\n                  `input_dim` when initializing this LSHash instance', ValueError('shapes (32,1000) and (2588,1000) not aligned: 1000 (dim 1) != 2588 (dim 0)',))\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "shapes (32,1000) and (2588,1000) not aligned: 1000 (dim 1) != 2588 (dim 0)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-36-624b4d465817>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlsh\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mLSHash\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlsh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/lshash/lshash.pyc\u001b[0m in \u001b[0;36mindex\u001b[1;34m(self, input_point, extra_data)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhash_tables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             table.append_val(self._hash(self.uniform_planes[i], input_point),\n\u001b[0m\u001b[0;32m    205\u001b[0m                              value)\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/lshash/lshash.pyc\u001b[0m in \u001b[0;36m_hash\u001b[1;34m(self, planes, input_point)\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0minput_point\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_point\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# for faster dot product\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0mprojections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplanes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_point\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             print(\"\"\"The input point needs to be an array-like object with\n",
        "\u001b[1;31mValueError\u001b[0m: shapes (32,1000) and (2588,1000) not aligned: 1000 (dim 1) != 2588 (dim 0)"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(reduced_matrix.getrow(0).indices)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "48\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Y=[]\n",
      "docs=[]\n",
      "for doc in list(abstract_db.find({},{'abstract':1})):\n",
      "    docs.extend(doc['abstract'])\n",
      "    Y.append(doc['_id'])\n",
      "\n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "vect = HashingVectorizer(non_negative=True)\n",
      "list_of_rows = []\n",
      "for doc in X:\n",
      "    sparse_row = vect.fit_transform([doc])\n",
      "    list_of_rows.append(sparse_row)\n",
      "\n",
      "print vstack(list_of_rows)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#online training of vectorizer\n",
      "hash_vect= HashingVectorizer(stop_words='english', ngram_range=(1,3), non_negative=True)\n",
      "list_of_rows = []\n",
      "for doc in docs:\n",
      "    sparse_row = hash_vect.fit_transform([doc])\n",
      "    list_of_rows.append(sparse_row)\n",
      "hash_stack = vstack(list_of_rows)\n",
      "h_transform = TfidfTransformer()\n",
      "hash_matrix = h_transform.fit_transform(hash_stack)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h_transform = TfidfTransformer()\n",
      "hash_matrix = h_transform.fit_transform(hash_stack)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kn =KNeighborsClassifier(algorithm= 'brute',metric='cosine')\n",
      "kn.fit(abstract_matrix,Y)\n",
      "hkn =KNeighborsClassifier(algorithm= 'brute',metric='cosine')\n",
      "hkn.fit(hash_matrix,Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='cosine',\n",
        "           metric_params=None, n_neighbors=5, p=2, weights='uniform')"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_tex='Consumption of calorie-containing sugars elicits appetitive behavioral responses and dopamine release in the ventral striatum, even in the absence of sweet-taste transduction machinery. However, it is unclear if such reward-related postingestive effects reflect preabsorptive or postabsorptive events. In support of the importance of postabsorptive glucose detection, we found that, in rat behavioral tests, high concentration glucose solutions administered in the jugular vein were sufficient to condition a side-bias. Additionally, a lower concentration glucose solution conditioned robust behavioral responses when administered in the hepatic-portal, but not the jugular vein. Furthermore, enteric administration of glucose at a concentration that is sufficient to elicit behavioral conditioning resulted in a glycemic profile similar to that observed after administration of the low concentration glucose solution in the hepatic-portal, but not jugular vein. Finally using fast-scan cyclic voltammetry we found that, in accordance with behavioral findings, a low concentration glucose solution caused an increase in spontaneous dopamine release events in the nucleus accumbens shell when administered in the hepatic-portal, but not the jugular vein. These findings demonstrate that the postabsorptive effects of glucose are sufficient for the postingestive behavioral and dopaminergic reward-related responses that result from sugar consumption. Furthermore, glycemia levels in the hepatic-portal venous system contribute more significantly for this effect than systemic glycemia, arguing for the participation of an intra-abdominal visceral sensor for glucose.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "foo = abstract_vectorizer.transform(test_tex)\n",
      "h_foo = h_transform.transform(hash_vect.transform(test_tex))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results=kn.kneighbors(foo,n_neighbors=15)\n",
      "h_results =hkn.kneighbors(h_foo,n_neighbors=15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print results[1]\n",
      "print\n",
      "print h_results[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1729 1730 1722 ..., 1737 1736 1719]\n",
        " [1729 1730 1722 ..., 1737 1736 1719]\n",
        " [1729 1730 1722 ..., 1737 1736 1719]\n",
        " ..., \n",
        " [1729 1730 1722 ..., 1737 1736 1719]\n",
        " [1729 1730 1722 ..., 1737 1736 1719]\n",
        " [1729 1730 1722 ..., 1737 1736 1719]]\n",
        "\n",
        "[[1729 1730 1722 ..., 1737 1736 1719]\n",
        " [1729 1730 1722 ..., 1737 1736 1719]\n",
        " [1729 1730 1722 ..., 1737 1736 1719]\n",
        " ..., \n",
        " [1729 1730 1722 ..., 1737 1736 1719]\n",
        " [1729 1730 1722 ..., 1737 1736 1719]\n",
        " [1729 1730 1722 ..., 1737 1736 1719]]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abstract_vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,3))\n",
      "abstract_matrix =abstract_vectorizer.fit_transform(docs)\n",
      "#save vectorizer\n",
      "#joblib.dump(abstract_vectorizer,'fit_vectorizer.pkl')\n",
      "#kn =KNeighborsClassifier(algorithm= 'brute',metric='cosine')\n",
      "#kn.fit(abstract_matrix,Y)\n",
      "#save knn\n",
      "#joblib.dump(kn,'abstract_neighbors.pkl')\n",
      "#save y labels\n",
      "#pickle.dump(Y,open(\"knn_ids.pkl\",\"wb\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start = time.time()\n",
      "cosine_pca= KernelPCA(n_components=1000,kernel='cosine')\n",
      "topics = cosine_pca.fit_transform(abstract_matrix)\n",
      "stop =time.time()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print start-stop"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-14.0195150375\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start = time.time()\n",
      "cosine_pca= TruncatedSVD(n_components=100)\n",
      "topics = cosine_pca.fit_transform(abstract_matrix)\n",
      "stop =time.time()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print start-stop"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-24.7120440006\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "print sys.getsizeof(docs), len(docs)\n",
      "tota_size=0\n",
      "for doc in docs:\n",
      "    tota_size+= sys.getsizeof(doc)\n",
      "print tota_size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "21056 2588\n",
        "19413796\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "#topic_kn=KNeighborsClassifier(algorithm= 'brute',metric='cosine')\n",
      "#topic_kn.fit(topics,Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='cosine',\n",
        "           metric_params=None, n_neighbors=5, p=2, weights='uniform')"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#joblib.dump(cosine_pca,'cosine_pca.pkl')\n",
      "#joblib.dump(topic_kn,'topic_kn.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "['topic_kn.pkl',\n",
        " 'topic_kn.pkl_01.npy',\n",
        " 'topic_kn.pkl_02.npy',\n",
        " 'topic_kn.pkl_03.npy']"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t_svd = TruncatedSVD(n_components=100)\n",
      "t_topics = t_svd.fit_transform(abstract_matrix)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t_kn =KNeighborsClassifier(algorithm= 'brute',metric='cosine')\n",
      "t_kn.fit(t_topics,Y)\n",
      "joblib.dump(t_svd,'t_svd.pkl')\n",
      "joblib.dump(t_kn,'t_kn.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "['t_kn.pkl', 't_kn.pkl_01.npy', 't_kn.pkl_02.npy', 't_kn.pkl_03.npy']"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results=kn.kneighbors(x,n_neighbors=5)\n",
      "results[1][0][1]\n",
      "for i in results[1][0]:\n",
      "    print type(Y[i])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<class 'bson.objectid.ObjectId'>\n",
        "<class 'bson.objectid.ObjectId'>\n",
        "<class 'bson.objectid.ObjectId'>\n",
        "<class 'bson.objectid.ObjectId'>\n",
        "<class 'bson.objectid.ObjectId'>\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abstract_2015_db.find_one({\"_id\":ObjectId('54f613e9387cca7aa63d7230')})['abstract']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "[u'\\nThe plant hormone abscisic acid (ABA) is present and active in humans, regulating glucose homeostasis. In normal glucose tolerant (NGT) human subjects, plasma ABA (ABAp) increases 5-fold after an oral glucose load. The aim of this study was to assess the effect of an oral glucose load on ABAp in type 2 diabetes (T2D) subjects. We chose two sub-groups of patients who underwent an oral glucose load for diagnostic purposes: i) 9 treatment-naive T2D subjects, and ii) 9 pregnant women with gestational diabetes (GDM), who underwent the glucose load before and 8\\u201312 weeks after childbirth. Each group was compared with matched NGT controls. The increase of ABAp in response to glucose was found to be abrogated in T2D patients compared to NGT controls. A similar result was observed in the women with GDM compared to pregnant NGT controls; 8\\u201312 weeks after childbirth, however, fasting ABAp and ABAp response to glucose were restored to normal in the GDM subjects, along with glucose tolerance. We also retrospectively compared fasting ABAp before and after bilio-pancreatic diversion (BPD) in obese, but not diabetic subjects, and in obese T2D patients, in which BPD resulted in the resolution of diabetes. Compared to pre-BPD values, basal ABAp significantly increased 1 month after BPD in T2D as well as in NGT subjects, in parallel with a reduction of fasting plasma glucose. These results indicate an impaired hyperglycemia-induced ABAp increase in T2D and in GDM and suggest a beneficial effect of elevated ABAp on glycemic control.\\n']"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "(array([[ 0.84259396,  0.91294923,  0.91953584,  0.92759215,  0.93725631]]),\n",
        " array([[1564, 2533,  579,  577, 1244]]))"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def search(abstract_text,N=10,method='tfidf_e'):\n",
      "    \"\"\"returns the N closets documents to your abstract text. Method speciifes what metric and space you search.\"\"\" \n",
      "    vectorize =  abstract_vectorizer\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kn =KNeighborsClassifier()\n",
      "kn.fit(stuff,Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "           metric_params=None, n_neighbors=5, p=2, weights='uniform')"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 760247)\t0.0455447371169\n",
        "  (0, 760182)\t0.0239533303482\n",
        "  (0, 760110)\t0.0226165684359\n",
        "  (0, 757263)\t0.0455447371169\n",
        "  (0, 757262)\t0.0455447371169\n",
        "  (0, 757261)\t0.0455447371169\n",
        "  (0, 757260)\t0.0455447371169\n",
        "  (0, 757257)\t0.0455447371169\n",
        "  (0, 757256)\t0.0455447371169\n",
        "  (0, 757255)\t0.0910894742338\n",
        "  (0, 757254)\t0.0455447371169\n",
        "  (0, 757253)\t0.0455447371169\n",
        "  (0, 757248)\t0.0455447371169\n",
        "  (0, 757247)\t0.0455447371169\n",
        "  (0, 757246)\t0.0455447371169\n",
        "  (0, 757245)\t0.0455447371169\n",
        "  (0, 757244)\t0.283039238242\n",
        "  (0, 756796)\t0.0455447371169\n",
        "  (0, 756795)\t0.0455447371169\n",
        "  (0, 756794)\t0.0865665563822\n",
        "  (0, 756704)\t0.0455447371169\n",
        "  (0, 756701)\t0.0416787475483\n",
        "  (0, 756578)\t0.0686935310905\n",
        "  (0, 741241)\t0.0455447371169\n",
        "  (0, 741240)\t0.0455447371169\n",
        "  :\t:\n",
        "  (0, 108338)\t0.0455447371169\n",
        "  (0, 108329)\t0.0455447371169\n",
        "  (0, 108328)\t0.0455447371169\n",
        "  (0, 108327)\t0.0833574950967\n",
        "  (0, 99413)\t0.0455447371169\n",
        "  (0, 99412)\t0.0455447371169\n",
        "  (0, 99350)\t0.0291222936619\n",
        "  (0, 89951)\t0.0455447371169\n",
        "  (0, 89950)\t0.0455447371169\n",
        "  (0, 89888)\t0.0308255427044\n",
        "  (0, 54421)\t0.0455447371169\n",
        "  (0, 54420)\t0.0455447371169\n",
        "  (0, 54419)\t0.0432832781911\n",
        "  (0, 54400)\t0.0455447371169\n",
        "  (0, 54399)\t0.0455447371169\n",
        "  (0, 54390)\t0.0394172886225\n",
        "  (0, 48474)\t0.0455447371169\n",
        "  (0, 48472)\t0.0432832781911\n",
        "  (0, 48469)\t0.0394172886225\n",
        "  (0, 35652)\t0.0455447371169\n",
        "  (0, 35651)\t0.0455447371169\n",
        "  (0, 35493)\t0.02469809421\n",
        "  (0, 31054)\t0.0455447371169\n",
        "  (0, 31053)\t0.0455447371169\n",
        "  (0, 31021)\t0.0378127579798\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results[1][0][1]\n",
      "for i in results[1][0]:\n",
      "    print Y[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "54f6137f387cca7aa63d6e67\n",
        "54f613e9387cca7aa63d7230\n",
        "54f6125b387cca7aa63d6a8d\n",
        "54f6125b387cca7aa63d6a8a\n",
        "54f61339387cca7aa63d6d27\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abstract_2015_db.find_one({\"_id\":ObjectId('54f6137f387cca7aa63d6e67')})['abstract']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "[u'Objectives: The placental transfer of nutrients is influenced by maternal metabolic state, placenta function and fetal demands. Human in vivo studies of this interplay are scarce and challenging. We aimed to establish a method to study placental nutrient transfer in humans. Focusing on glucose, we tested a hypothesis that maternal glucose concentrations and uteroplacental arterio-venous difference (reflecting maternal supply) determines the fetal venous-arterial glucose difference (reflecting fetal consumption). Methods: Cross-sectional in vivo study of 40 healthy women with uncomplicated term pregnancies undergoing planned caesarean section. Glucose and insulin were measured in plasma from maternal and fetal sides of the placenta, at the incoming (radial artery and umbilical vein) and outgoing vessels (uterine vein and umbilical artery). Results: There were significant mean (SD) uteroplacental arterio-venous 0.29 (0.23) mmol/L and fetal venous-arterial 0.38 (0.31) mmol/L glucose differences. The transplacental maternal-fetal glucose gradient was 1.22 (0.42) mmol/L. The maternal arterial glucose concentration was correlated to the fetal venous glucose concentration (r = 0.86, p<0.001), but not to the fetal venous-arterial glucose difference. The uteroplacental arterio-venous glucose difference was neither correlated to the level of glucose in the umbilical vein, nor fetal venous-arterial glucose difference. The maternal-fetal gradient was correlated to fetal venous-arterial glucose difference (r = 0.8, p<0.001) and the glucose concentration in the umbilical artery (r = \\u22120.45, p = 0.004). Glucose and insulin concentrations were correlated in the mother (r = 0.52, p = 0.001), but not significantly in the fetus. We found no significant correlation between maternal and fetal insulin values. Conclusions: We did not find a relation between indicators of maternal glucose supply and the fetal venous-arterial glucose difference. Our findings indicate that the maternal-fetal glucose gradient is significantly influenced by the fetal venous-arterial difference and not merely dependent on maternal glucose concentration or the arterio-venous difference on the maternal side of the placenta. ']"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docs=[]\n",
      "for doc in list(abstract_2015_db.find({},{'abstract':1})):\n",
      "    docs.extend(doc['abstract'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stuff =clf.transform(docs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abstract_2015_db.find({},{'abstract':1}).next()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "{u'_id': ObjectId('54f611db387cca7aa63d684c'),\n",
        " u'abstract': [u'\\nMost of the reproductive modes of frogs include an exotrophic tadpole, but a number of taxa have some form of endotrophic development that lacks a feeding tadpole stage. The dicroglossid frog genus Limnonectes ranges from China south into Indonesia. The breeding biologies of the approximately 60 described species display an unusual diversity that range from exotrophic tadpoles to endotrophic development in terrestrial nests. There have been mentions of oviductal production of typical, exotrophic tadpoles in an undescribed species of Limnonectes from Sulawesi, Indonesia. Here we examine newly collected specimens of this species, now described as L. larvaepartus and present the first substantial report on this unique breeding mode. Typical exotrophic tadpoles that are retained to an advanced developmental stage in the oviducts of a female frog are birthed into slow-flowing streams or small, non-flowing pools adjacent to the streams.\\n']}"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bar"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 56,
       "text": [
        "<2588x767976 sparse matrix of type '<type 'numpy.float64'>'\n",
        "\twith 1101013 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abstract_vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,3))\n",
      "bar = abstract_vectorizer.fit_transform(docs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(bar,'tfidf_2015.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'str' object has no attribute 'write'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-55-21d4c078015c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'tfidf_2015.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m     \u001b[0mPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, protocol)\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pickle protocol must be <= %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'write'"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "abstract_vectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "TfidfVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error=u'strict',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
        "        ngram_range=(1, 3), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
        "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
        "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
        "        vocabulary=None)"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://stackoverflow.com/questions/18089598/is-there-a-way-to-store-python-objects-directly-in-mongodb-without-serializing-t"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for doc in list(abstract_2015_db.find({},{'abstract':1})):\n",
      "    Vector = abstract_vectorizer.transform(doc['abstract'])\n",
      "    abstract_2015_db.update(doc,\n",
      "                            {\"$set\":{\"vector\":Vector}},False,True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "InvalidDocument",
       "evalue": "Cannot encode object: <1x767976 sparse matrix of type '<type 'numpy.float64'>'\n\twith 222 stored elements in Compressed Sparse Row format>",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mInvalidDocument\u001b[0m                           Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-48-64a1f0f0c19e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mVector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabstract_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     abstract_2015_db.update(doc,\n\u001b[1;32m----> 4\u001b[1;33m                             {\"$set\":{\"vector\":Vector}},False,True)\n\u001b[0m",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pymongo/collection.pyc\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, spec, document, upsert, manipulate, safe, multi, check_keys, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m             results = message._do_batched_write_command(\n\u001b[0;32m    550\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.$cmd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_UPDATE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 docs, check_keys, self.uuid_subtype, client)\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[0m_check_write_command_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mInvalidDocument\u001b[0m: Cannot encode object: <1x767976 sparse matrix of type '<type 'numpy.float64'>'\n\twith 222 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "joblib.dump(abstract_vectorizer,'fit_vectorizer.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "['fit_vectorizer.pkl',\n",
        " 'fit_vectorizer.pkl_01.npy',\n",
        " 'fit_vectorizer.pkl_02.npy']"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(abstract_vectorizer.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "767976"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf=joblib.load('fit_vectorizer.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x= abstract_vectorizer.transform(['Consumption of calorie-containing sugars elicits appetitive behavioral responses and dopamine release in the ventral striatum, even in the absence of sweet-taste transduction machinery. However, it is unclear if such reward-related postingestive effects reflect preabsorptive or postabsorptive events. In support of the importance of postabsorptive glucose detection, we found that, in rat behavioral tests, high concentration glucose solutions administered in the jugular vein were sufficient to condition a side-bias. Additionally, a lower concentration glucose solution conditioned robust behavioral responses when administered in the hepatic-portal, but not the jugular vein. Furthermore, enteric administration of glucose at a concentration that is sufficient to elicit behavioral conditioning resulted in a glycemic profile similar to that observed after administration of the low concentration glucose solution in the hepatic-portal, but not jugular vein. Finally using fast-scan cyclic voltammetry we found that, in accordance with behavioral findings, a low concentration glucose solution caused an increase in spontaneous dopamine release events in the nucleus accumbens shell when administered in the hepatic-portal, but not the jugular vein. These findings demonstrate that the postabsorptive effects of glucose are sufficient for the postingestive behavioral and dopaminergic reward-related responses that result from sugar consumption. Furthermore, glycemia levels in the hepatic-portal venous system contribute more significantly for this effect than systemic glycemia, arguing for the participation of an intra-abdominal visceral sensor for glucose.'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import searcher_one"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "searcher_one."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}